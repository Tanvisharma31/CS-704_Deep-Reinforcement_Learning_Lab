{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# Function to create shallow or deep network model\n",
    "def create_model(layers=3, activation='sigmoid'):\n",
    "    model = Sequential([Flatten(input_shape=(28, 28))])\n",
    "    for _ in range(layers):\n",
    "        model.add(Dense(128, activation=activation))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# Function to train and record gradient values\n",
    "def train_and_record_gradients(model, epochs=5):\n",
    "    sgd = SGD(learning_rate=0.01)\n",
    "    model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    gradient_magnitudes = []\n",
    "\n",
    "    # Custom training loop to record gradient magnitudes\n",
    "    for epoch in range(epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(X_train, training=True)\n",
    "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_train, predictions))\n",
    "        \n",
    "        # Get gradients and compute their magnitudes\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        grad_norms = [tf.norm(g).numpy() for g in gradients if g is not None]\n",
    "        avg_grad_norm = np.mean(grad_norms)\n",
    "        gradient_magnitudes.append(avg_grad_norm)\n",
    "        print(f\"Epoch {epoch+1}, Avg Gradient Norm: {avg_grad_norm}\")\n",
    "\n",
    "        model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "    return gradient_magnitudes\n",
    "\n",
    "# Experiment with shallow and deep networks with sigmoid and ReLU activations\n",
    "results = {}\n",
    "\n",
    "for architecture in ['shallow', 'deep']:\n",
    "    for activation in ['sigmoid', 'relu']:\n",
    "        print(f\"\\nTraining {architecture} network with {activation} activation:\")\n",
    "        layers = 3 if architecture == 'shallow' else 10  # Shallow: 3 layers, Deep: 10 layers\n",
    "        model = create_model(layers=layers, activation=activation)\n",
    "        gradient_magnitudes = train_and_record_gradients(model, epochs=10)\n",
    "        results[f\"{architecture}_{activation}\"] = gradient_magnitudes\n",
    "\n",
    "# Plotting the gradient magnitudes to observe vanishing and exploding gradients\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for key, gradients in results.items():\n",
    "    plt.plot(gradients, label=key)\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Gradient Magnitude')\n",
    "plt.title('Gradient Magnitude Across Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
