{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D, BatchNormalization, Layer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Custom Layer for Instance Normalization\n",
    "class InstanceNormalization(Layer):\n",
    "    def __init__(self, epsilon=1e-5):\n",
    "        super(InstanceNormalization, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name='gamma', shape=(input_shape[-1],),\n",
    "                                     initializer=\"ones\", trainable=True)\n",
    "        self.beta = self.add_weight(name='beta', shape=(input_shape[-1],),\n",
    "                                    initializer=\"zeros\", trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n",
    "        normalized = (x - mean) / tf.sqrt(variance + self.epsilon)\n",
    "        return self.gamma * normalized + self.beta\n",
    "\n",
    "# Function to build a CNN model with either BatchNorm or InstanceNorm\n",
    "def build_model(normalization='batch'):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        BatchNormalization() if normalization == 'batch' else InstanceNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        BatchNormalization() if normalization == 'batch' else InstanceNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        BatchNormalization() if normalization == 'batch' else InstanceNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Compile and train models\n",
    "def train_model(model, epochs=10):\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=64, validation_data=(X_test, y_test))\n",
    "    return history\n",
    "\n",
    "# Build and train models with BatchNorm and InstanceNorm\n",
    "batch_norm_model = build_model(normalization='batch')\n",
    "instance_norm_model = build_model(normalization='instance')\n",
    "\n",
    "print(\"Training model with Batch Normalization...\")\n",
    "batch_norm_history = train_model(batch_norm_model, epochs=10)\n",
    "\n",
    "print(\"\\nTraining model with Instance Normalization...\")\n",
    "instance_norm_history = train_model(instance_norm_model, epochs=10)\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(batch_norm_history.history['accuracy'], label='BatchNorm Train')\n",
    "plt.plot(batch_norm_history.history['val_accuracy'], label='BatchNorm Val')\n",
    "plt.plot(instance_norm_history.history['accuracy'], label='InstanceNorm Train')\n",
    "plt.plot(instance_norm_history.history['val_accuracy'], label='InstanceNorm Val')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(batch_norm_history.history['loss'], label='BatchNorm Train')\n",
    "plt.plot(batch_norm_history.history['val_loss'], label='BatchNorm Val')\n",
    "plt.plot(instance_norm_history.history['loss'], label='InstanceNorm Train')\n",
    "plt.plot(instance_norm_history.history['val_loss'], label='InstanceNorm Val')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
